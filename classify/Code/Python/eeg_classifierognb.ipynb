{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PLF48faHpeBU",
    "outputId": "fe56cdb0-41d8-4456-8bb6-a6b7cdf60bc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting mne\n",
      "  Downloading mne-1.8.0-py3-none-any.whl (7.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hCollecting lazy-loader>=0.3\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Collecting scipy>=1.9\n",
      "  Downloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: decorator in /home/ubie/.local/lib/python3.10/site-packages (from mne) (5.1.1)\n",
      "Collecting numpy<3,>=1.23\n",
      "  Downloading numpy-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting jinja2\n",
      "  Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.3/133.3 KB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /home/ubie/.local/lib/python3.10/site-packages (from mne) (24.2)\n",
      "Collecting matplotlib>=3.6\n",
      "  Downloading matplotlib-3.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting pooch>=1.5\n",
      "  Downloading pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.6/64.6 KB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading tqdm-4.67.0-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 KB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (324 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.0/325.0 KB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1\n",
      "  Downloading kiwisolver-1.4.7-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7 in /home/ubie/.local/lib/python3.10/site-packages (from matplotlib>=3.6->mne) (2.9.0.post0)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.55.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib>=3.6->mne) (2.4.7)\n",
      "Requirement already satisfied: pillow>=8 in /usr/lib/python3/dist-packages (from matplotlib>=3.6->mne) (9.0.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/lib/python3/dist-packages (from pooch>=1.5->mne) (2.25.1)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /home/ubie/.local/lib/python3.10/site-packages (from pooch>=1.5->mne) (4.3.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib/python3/dist-packages (from jinja2->mne) (2.0.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6->mne) (1.16.0)\n",
      "Installing collected packages: tqdm, pooch, numpy, lazy-loader, kiwisolver, jinja2, fonttools, cycler, scipy, contourpy, matplotlib, mne\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.55.0 jinja2-3.1.4 kiwisolver-1.4.7 lazy-loader-0.4 matplotlib-3.9.2 mne-1.8.0 numpy-2.1.3 pooch-1.8.2 scipy-1.14.1 tqdm-4.67.0\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /home/ubie/.local/lib/python3.10/site-packages (2.1.3)\n",
      "Requirement already satisfied: scipy in /home/ubie/.local/lib/python3.10/site-packages (1.14.1)\n",
      "Requirement already satisfied: matplotlib in /home/ubie/.local/lib/python3.10/site-packages (3.9.2)\n",
      "Collecting latexify-py\n",
      "  Downloading latexify_py-0.4.3.post1-py3-none-any.whl (38 kB)\n",
      "Collecting skfeature-chappers\n",
      "  Downloading skfeature_chappers-1.1.0-py3-none-any.whl (66 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.3/66.3 KB\u001b[0m \u001b[31m343.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ubie/.local/lib/python3.10/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ubie/.local/lib/python3.10/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ubie/.local/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ubie/.local/lib/python3.10/site-packages (from matplotlib) (4.55.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ubie/.local/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: pillow>=8 in /usr/lib/python3/dist-packages (from matplotlib) (9.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubie/.local/lib/python3.10/site-packages (from matplotlib) (24.2)\n",
      "Collecting dill>=0.3.2\n",
      "  Downloading dill-0.3.9-py3-none-any.whl (119 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 KB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pandas\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->skfeature-chappers) (2022.1)\n",
      "Collecting tzdata>=2022.7\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.6/346.6 KB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=3.1.0\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Collecting joblib>=1.2.0\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 KB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tzdata, threadpoolctl, joblib, dill, scikit-learn, pandas, latexify-py, skfeature-chappers\n",
      "Successfully installed dill-0.3.9 joblib-1.4.2 latexify-py-0.4.3.post1 pandas-2.2.3 scikit-learn-1.5.2 skfeature-chappers-1.1.0 threadpoolctl-3.5.0 tzdata-2024.2\n"
     ]
    }
   ],
   "source": [
    "!pip install mne\n",
    "# Install required Python libraries for EEG processing and machine learning\n",
    "!pip install numpy scipy matplotlib latexify-py skfeature-chappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "oDGssEyanoPK"
   },
   "outputs": [],
   "source": [
    "import os  # Built-in Python module, no need to install\n",
    "import mne\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "# Ignore warnings for a cleaner output\n",
    "warnings.filterwarnings(\"ignore\", message=\".*annotation.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FHNLNVriqsMS",
    "outputId": "b263711e-d951-4f80-988d-6fafe2f3ed55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pC-djc6drvYY",
    "outputId": "99b9ca35-e6fc-422c-8423-5335fa4f7a74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found files: ['Subject3-[2012.04.07-18.27.18].gdf', 'Subject5-[2012.04.09-19.48.56].gdf', 'Subject2-[2012.04.07-19.44.23].gdf', 'Subject4-[2012.04.08-16.06.48].gdf', 'Subject2-[2012.04.07-19.36.29].gdf', 'Subject2-[2012.04.07-19.57.52].gdf', 'Subject3-[2012.04.07-18.45.34].gdf', 'Subject3-[2012.04.07-18.17.50].gdf', 'Subject3-[2012.04.07-18.53.10].gdf', 'Subject5-[2012.04.09-20.02.48].gdf', 'Subject4-[2012.04.08-16.19.25].gdf', 'Subject2-[2012.04.07-19.27.02].gdf', 'Subject5-[2012.04.09-19.38.11].gdf', 'Subject4-[2012.04.08-16.27.27].gdf', 'Subject4-[2012.04.08-16.35.27].gdf', 'Subject5-[2012.04.09-19.56.38].gdf']\n",
      "Full paths: ['/home/ubie/Desktop/Telekinesis/classify/Data/Subject3-[2012.04.07-18.27.18].gdf', '/home/ubie/Desktop/Telekinesis/classify/Data/Subject5-[2012.04.09-19.48.56].gdf', '/home/ubie/Desktop/Telekinesis/classify/Data/Subject2-[2012.04.07-19.44.23].gdf', '/home/ubie/Desktop/Telekinesis/classify/Data/Subject4-[2012.04.08-16.06.48].gdf', '/home/ubie/Desktop/Telekinesis/classify/Data/Subject2-[2012.04.07-19.36.29].gdf', '/home/ubie/Desktop/Telekinesis/classify/Data/Subject2-[2012.04.07-19.57.52].gdf', '/home/ubie/Desktop/Telekinesis/classify/Data/Subject3-[2012.04.07-18.45.34].gdf', '/home/ubie/Desktop/Telekinesis/classify/Data/Subject3-[2012.04.07-18.17.50].gdf', '/home/ubie/Desktop/Telekinesis/classify/Data/Subject3-[2012.04.07-18.53.10].gdf', '/home/ubie/Desktop/Telekinesis/classify/Data/Subject5-[2012.04.09-20.02.48].gdf', '/home/ubie/Desktop/Telekinesis/classify/Data/Subject4-[2012.04.08-16.19.25].gdf', '/home/ubie/Desktop/Telekinesis/classify/Data/Subject2-[2012.04.07-19.27.02].gdf', '/home/ubie/Desktop/Telekinesis/classify/Data/Subject5-[2012.04.09-19.38.11].gdf', '/home/ubie/Desktop/Telekinesis/classify/Data/Subject4-[2012.04.08-16.27.27].gdf', '/home/ubie/Desktop/Telekinesis/classify/Data/Subject4-[2012.04.08-16.35.27].gdf', '/home/ubie/Desktop/Telekinesis/classify/Data/Subject5-[2012.04.09-19.56.38].gdf']\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"/home/ubie/Desktop/Telekinesis/classify/Data\"\n",
    "\n",
    "def data_path(folder_path, data_format=\"gdf\"):\n",
    "    path_files = []  # Store paths of matching files\n",
    "    files = []  # Store file names\n",
    "    folders = []  # Store folder names\n",
    "\n",
    "    # Walk through the directory and collect relevant files/folders\n",
    "    for root, dirnames, filenames in os.walk(folder_path):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith(f\".{data_format}\"):\n",
    "                full_path = os.path.join(root, filename)\n",
    "                path_files.append(full_path)\n",
    "                files.append(filename)\n",
    "        folders.extend(dirnames)\n",
    "\n",
    "    return path_files, files, folders\n",
    "\n",
    "# Get the paths of all GDF files\n",
    "path_files, files, folders = data_path(folder_path, data_format=\"gdf\")\n",
    "\n",
    "# Print the collected file paths\n",
    "print(\"Found files:\", files)\n",
    "print(\"Full paths:\", path_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ld7wRtp1tgL5",
    "outputId": "35fab330-dc0d-4969-a9d0-bde5701b2ef2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GDF file: /home/ubie/Desktop/Telekinesis/classify/Data/Subject3-[2012.04.07-18.27.18].gdf\n",
      "Data: (86656, 8) \n",
      "\n",
      "Channels Name: ['Oz', 'O1', 'O2', 'PO3', 'POz', 'PO7', 'PO8', 'PO4'] \n",
      "\n",
      "Labels: ['32769' '33024' '32779' '32780' '33026' '32779' '32780' '33027' '32779'\n",
      " '32780' '33025' '32779' '32780' '33026' '32779' '32780' '33025' '32779'\n",
      " '32780' '33024' '32779' '32780' '33027' '32779' '32780' '33025' '32779'\n",
      " '32780' '33026' '32779' '32780' '33027' '32779' '32780' '33024' '32779'\n",
      " '32780' '33026' '32779' '32780' '33024' '32779' '32780' '33027' '32779'\n",
      " '32780' '33025' '32779' '32780' '33024' '32779' '32780' '33027' '32779'\n",
      " '32780' '33025' '32779' '32780' '33026' '32779' '32780' '33027' '32779'\n",
      " '32780' '33024' '32779' '32780' '33025' '32779' '32780' '33026' '32779'\n",
      " '32780' '33025' '32779' '32780' '33027' '32779' '32780' '33026' '32779'\n",
      " '32780' '33024' '32779' '32780' '33027' '32779' '32780' '33026' '32779'\n",
      " '32780' '33024' '32779' '32780' '33025' '32779' '32780' '33024' '32779'\n",
      " '32780' '33026' '32779' '32780' '33027' '32779' '32780' '33025' '32779'\n",
      " '32780' '33024' '32779' '32780' '33026' '32779' '32780' '33027' '32779'\n",
      " '32780' '33025' '32779' '32780' '32770'] \n",
      "\n",
      "Events: [[ 4170     0     1]\n",
      " [ 4682     0     5]\n",
      " [ 4938     0     3]\n",
      " [ 6218     0     4]\n",
      " [ 6730     0     7]\n",
      " [ 6986     0     3]\n",
      " [ 8266     0     4]\n",
      " [ 8778     0     8]\n",
      " [ 9034     0     3]\n",
      " [10314     0     4]\n",
      " [10826     0     6]\n",
      " [11082     0     3]\n",
      " [12362     0     4]\n",
      " [12874     0     7]\n",
      " [13130     0     3]\n",
      " [14410     0     4]\n",
      " [14922     0     6]\n",
      " [15178     0     3]\n",
      " [16458     0     4]\n",
      " [16970     0     5]\n",
      " [17226     0     3]\n",
      " [18506     0     4]\n",
      " [19018     0     8]\n",
      " [19274     0     3]\n",
      " [20554     0     4]\n",
      " [21066     0     6]\n",
      " [21322     0     3]\n",
      " [22602     0     4]\n",
      " [23114     0     7]\n",
      " [23370     0     3]\n",
      " [24650     0     4]\n",
      " [25162     0     8]\n",
      " [25418     0     3]\n",
      " [26698     0     4]\n",
      " [27210     0     5]\n",
      " [27466     0     3]\n",
      " [28746     0     4]\n",
      " [29258     0     7]\n",
      " [29514     0     3]\n",
      " [30794     0     4]\n",
      " [31306     0     5]\n",
      " [31562     0     3]\n",
      " [32842     0     4]\n",
      " [33354     0     8]\n",
      " [33610     0     3]\n",
      " [34890     0     4]\n",
      " [35402     0     6]\n",
      " [35658     0     3]\n",
      " [36938     0     4]\n",
      " [37450     0     5]\n",
      " [37706     0     3]\n",
      " [38986     0     4]\n",
      " [39498     0     8]\n",
      " [39754     0     3]\n",
      " [41034     0     4]\n",
      " [41546     0     6]\n",
      " [41802     0     3]\n",
      " [43082     0     4]\n",
      " [43594     0     7]\n",
      " [43850     0     3]\n",
      " [45130     0     4]\n",
      " [45642     0     8]\n",
      " [45898     0     3]\n",
      " [47178     0     4]\n",
      " [47690     0     5]\n",
      " [47946     0     3]\n",
      " [49226     0     4]\n",
      " [49738     0     6]\n",
      " [49994     0     3]\n",
      " [51274     0     4]\n",
      " [51786     0     7]\n",
      " [52042     0     3]\n",
      " [53322     0     4]\n",
      " [53834     0     6]\n",
      " [54090     0     3]\n",
      " [55370     0     4]\n",
      " [55882     0     8]\n",
      " [56138     0     3]\n",
      " [57418     0     4]\n",
      " [57930     0     7]\n",
      " [58186     0     3]\n",
      " [59466     0     4]\n",
      " [59978     0     5]\n",
      " [60234     0     3]\n",
      " [61514     0     4]\n",
      " [62026     0     8]\n",
      " [62282     0     3]\n",
      " [63562     0     4]\n",
      " [64074     0     7]\n",
      " [64330     0     3]\n",
      " [65610     0     4]\n",
      " [66122     0     5]\n",
      " [66378     0     3]\n",
      " [67658     0     4]\n",
      " [68170     0     6]\n",
      " [68426     0     3]\n",
      " [69706     0     4]\n",
      " [70218     0     5]\n",
      " [70474     0     3]\n",
      " [71754     0     4]\n",
      " [72266     0     7]\n",
      " [72522     0     3]\n",
      " [73802     0     4]\n",
      " [74314     0     8]\n",
      " [74570     0     3]\n",
      " [75850     0     4]\n",
      " [76362     0     6]\n",
      " [76618     0     3]\n",
      " [77898     0     4]\n",
      " [78410     0     5]\n",
      " [78666     0     3]\n",
      " [79946     0     4]\n",
      " [80458     0     7]\n",
      " [80714     0     3]\n",
      " [81994     0     4]\n",
      " [82506     0     8]\n",
      " [82762     0     3]\n",
      " [84042     0     4]\n",
      " [84554     0     6]\n",
      " [84810     0     3]\n",
      " [86090     0     4]\n",
      " [86602     0     2]] \n",
      "\n",
      "Event Indices: {np.str_('32769'): 1, np.str_('32770'): 2, np.str_('32779'): 3, np.str_('32780'): 4, np.str_('33024'): 5, np.str_('33025'): 6, np.str_('33026'): 7, np.str_('33027'): 8} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ensure the path_files variable contains GDF file paths\n",
    "print(f\"Using GDF file: {path_files[0]}\")\n",
    "\n",
    "# Read the GDF file into a raw MNE object\n",
    "raw = mne.io.read_raw_gdf(path_files[0], verbose=0)\n",
    "\n",
    "# Extract the channel names from the raw data\n",
    "channels_name = raw.ch_names\n",
    "\n",
    "# Get the EEG data and transpose it to have channels as rows and samples as columns\n",
    "data = 1e6 * raw.get_data().T  # Convert to microvolts if necessary\n",
    "\n",
    "# Get the sampling frequency of the EEG data\n",
    "fs = raw.info['sfreq']\n",
    "\n",
    "# Extract labels from annotations\n",
    "labels = raw.annotations.description\n",
    "\n",
    "# Get the events and their corresponding indices\n",
    "events, event_ind = mne.events_from_annotations(raw, verbose=0)\n",
    "\n",
    "# Print all the relevant information\n",
    "print(f\"Data: {data.shape} \\n\")\n",
    "print(f\"Channels Name: {channels_name} \\n\")\n",
    "print(f\"Labels: {labels} \\n\")\n",
    "print(f\"Events: {events} \\n\")\n",
    "print(f\"Event Indices: {event_ind} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0ts4xdqwVqr"
   },
   "source": [
    "Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rb0zQAVTg3kO",
    "outputId": "687e807d-209a-416b-ec38-93e2fb78e9c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data1.shape: (1280, 8, 160) \n",
      "data2.shape: (1280, 8, 160) \n",
      "data3.shape: (1280, 8, 160)\n"
     ]
    }
   ],
   "source": [
    "# Define the duration of each trial in seconds\n",
    "time_trial = 5\n",
    "\n",
    "# Initialize lists to store trial data for each label\n",
    "data1, data2, data3 = [], [], []\n",
    "\n",
    "# Define the labels for each stimulation frequency (as strings)\n",
    "lab = ['33025', '33026', '33027'] \n",
    "\n",
    "# Create a list of the initialized data arrays to manage them easily\n",
    "data_list = [data1, data2, data3]\n",
    "\n",
    "# Loop through all GDF files in the given path\n",
    "for i in range(len(path_files)):\n",
    "    # Read the GDF file into an MNE raw object\n",
    "    raw = mne.io.read_raw_gdf(path_files[i], verbose=0)\n",
    "\n",
    "    # Get sampling frequency and channel names\n",
    "    fs = raw.info['sfreq']\n",
    "    channels_name = raw.ch_names\n",
    "\n",
    "    # Calculate the number of samples per trial\n",
    "    duration_trial = int(fs * time_trial)\n",
    "\n",
    "    # Extract EEG data and transpose (channels as rows, samples as columns)\n",
    "    data = 1e6 * raw.get_data().T  # Convert to microvolts if needed\n",
    "\n",
    "    # Extract labels from annotations\n",
    "    labels = np.array(raw.annotations.description)\n",
    "\n",
    "    # Extract events and their start times\n",
    "    events, _ = mne.events_from_annotations(raw, verbose=0)\n",
    "    time_start_trial = events[:, 0]  # Start times of trials\n",
    "\n",
    "    # Loop over the defined labels\n",
    "    for j, val in enumerate(lab):\n",
    "        # Find trials with the current label\n",
    "        num_trials = np.where(labels == val)[0]\n",
    "\n",
    "        # Initialize array to store data for this label\n",
    "        data_trial = np.zeros((duration_trial, len(channels_name), len(num_trials)))\n",
    "\n",
    "        # Extract data for each trial of the current label\n",
    "        for ind, trial_index in enumerate(num_trials):\n",
    "            start = time_start_trial[trial_index]\n",
    "            data_trial[:, :, ind] = data[start:start + duration_trial, :]\n",
    "\n",
    "        # Store the extracted trial data in the appropriate list\n",
    "        data_list[j].append(data_trial)\n",
    "\n",
    "# Concatenate all the data arrays along the third axis (trials)\n",
    "data1 = np.concatenate(data1, axis=2)\n",
    "data2 = np.concatenate(data2, axis=2)\n",
    "data3 = np.concatenate(data3, axis=2)\n",
    "\n",
    "# Print the shapes of the final concatenated data arrays\n",
    "print(f\"data1.shape: {data1.shape} \\ndata2.shape: {data2.shape} \\ndata3.shape: {data3.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IpEf3ezRFFDB"
   },
   "source": [
    "Filtering/ feature Extraction / Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qmB7YnMGCI2l",
    "outputId": "5b874d39-000e-4c61-c710-b6845ba414dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape : (1280, 8, 160)\n",
      "Filtered data shape: (1280, 8, 160)\n"
     ]
    }
   ],
   "source": [
    "#Import required libraries\n",
    "import numpy as np\n",
    "import mne\n",
    "import warnings\n",
    "import sys \n",
    "\n",
    "sys.path.append('/home/ubie/Desktop/Telekinesis/classify/Code/Python/Functions')\n",
    "\n",
    "# Import functions from the .py files in the Functions folder\n",
    "from Filtering import filtering\n",
    "\n",
    "# Step 4: Define your parameters for filtering\n",
    "trial = 8            # Define trial number (trial 1 in Python index starts from 0)\n",
    "order = 3            # Define filter order\n",
    "f_low = 0.05         # Define lower cutoff frequency for the bandpass filter (Hz)\n",
    "f_high = 100         # Define upper cutoff frequency for the bandpass filter (Hz)\n",
    "notch_freq = 50      # Define frequency to be removed from the signal for notch filter (Hz)\n",
    "quality_factor = 20  # Define quality factor for the notch filter\n",
    "notch_filter = \"on\"  # on or off\n",
    "filter_active = \"on\" # on or off\n",
    "design_method = \"IIR\" # IIR or FIR\n",
    "type_filter = \"bandpass\"  # low, high, bandpass, or bandstop\n",
    "freq_stim = 13       # Define stimulation frequency\n",
    "\n",
    "print(f\"Data shape : {data1.shape}\")\n",
    "\n",
    "# Step 5: Apply bandpass filtering to the EEG data\n",
    "filtered_data = filtering(data1, f_low, f_high, order, fs, notch_freq, quality_factor,\n",
    "                          filter_active, notch_filter, type_filter, design_method)\n",
    "\n",
    "# Print the shape of the filtered data to verify\n",
    "print(f\"Filtered data shape: {filtered_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7M3aFA-Wq3R"
   },
   "source": [
    "CAR Filter (Common average reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pH9pTa0ZCIpq",
    "outputId": "64346571-e7d1-4720-af75-cad5143d7fae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAR-filtered data shape: (1280, 8, 160)\n",
      "CAR-filtered data for Trial 1: (1280, 8)\n"
     ]
    }
   ],
   "source": [
    "#Import the CAR filter function\n",
    "from Common_average_reference import car\n",
    "\n",
    "# Step 3: Apply CAR filter to the data\n",
    "# filtered_data shape: (1280, 8, 160)\n",
    "data_car = car(filtered_data, reference_channel=None)  # Use all channels for average reference\n",
    "\n",
    "# Step 4: Verify the shape of CAR-filtered data\n",
    "print(f\"CAR-filtered data shape: {data_car.shape}\")\n",
    "\n",
    "# Step 5: Extract the trial-specific data if needed\n",
    "trial = 0  # Define trial number (0-indexed)\n",
    "trial_data_car = data_car[:, :, trial]  # Extract data for the selected trial\n",
    "\n",
    "# Step 6: Verify the trial data shape\n",
    "print(f\"CAR-filtered data for Trial {trial + 1}: {trial_data_car.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mesmd1Vc3HhI"
   },
   "source": [
    "CCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CQGdpBjm4ezb",
    "outputId": "faa2743c-d06f-4f54-eb9d-9df5ade030ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Data Shape: (1280, 8, 480)\n",
      "CAR-Filtered Data Shape: (1280, 8, 480)\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 2. 1.\n",
      " 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 0. 1. 2. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 2. 1. 1. 1. 0. 1. 1. 1. 2. 1. 1. 1. 1. 1. 2. 1. 1. 1. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      "Classification Accuracy: 92.08%\n"
     ]
    }
   ],
   "source": [
    "# Import functions\n",
    "from Filtering import filtering\n",
    "from Common_average_reference import car\n",
    "from CCA import cca\n",
    "\n",
    "# ------------------------------------ Step 2: Load and Combine Data ----------------------------------------\n",
    "# Assuming data1, data2, and data3 are already loaded\n",
    "data_total = np.concatenate((data1, data2, data3), axis=2)\n",
    "\n",
    "# Generate labels for each dataset (0, 1, 2)\n",
    "labels = np.concatenate((np.full(data1.shape[-1], 0),\n",
    "                         np.full(data2.shape[-1], 1),\n",
    "                         np.full(data3.shape[-1], 2)))\n",
    "\n",
    "# ------------------------------------ Step 3: Filter the Data ----------------------------------------\n",
    "order = 4\n",
    "f_low = 0.05\n",
    "f_high = 100\n",
    "notch_freq = 50\n",
    "quality_factor = 20\n",
    "notch_filter = \"on\"\n",
    "filter_active = \"off\"\n",
    "type_filter = \"bandpass\"\n",
    "\n",
    "filtered_data = filtering(data_total, f_low, f_high, order, fs,\n",
    "                          notch_freq, quality_factor, filter_active,\n",
    "                          notch_filter, type_filter)\n",
    "\n",
    "print(f\"Filtered Data Shape: {filtered_data.shape}\")\n",
    "\n",
    "# ----------------------------------- Step 4: Apply CAR Filter ----------------------------------------\n",
    "data_car = car(filtered_data)\n",
    "print(f\"CAR-Filtered Data Shape: {data_car.shape}\")\n",
    "\n",
    "# ----------------------------------- Step 5: Perform CCA Classification ----------------------------------------\n",
    "num_channel = [0, 1, 2]   # List of channels to use\n",
    "num_harmonic = 4          # Number of harmonics\n",
    "f_stim = [13, 21, 17]     # Frequencies used for stimulation\n",
    "\n",
    "# Use your CCA function to classify the EEG signals\n",
    "predict_label = cca(data_car, fs, f_stim, num_channel, num_harmonic)\n",
    "\n",
    "print(predict_label)\n",
    "\n",
    "# ------------------------------------ Step 6: Calculate Accuracy ----------------------------------------\n",
    "accuracy = np.sum(labels == predict_label) / len(predict_label) * 100\n",
    "print(f\"Classification Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkFhtNHJ-kmb"
   },
   "source": [
    "Feature Extraction using CCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ARyIJ-lY1n_z",
    "outputId": "6b852de0-e109-4ecc-dc3f-2348e0c40b27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Features:  [[1.55465861e-01 3.00801634e-02 6.13251260e-04 ... 5.63965316e-03\n",
      "  9.22320366e-04 1.39542014e-04]\n",
      " [1.90205686e-01 7.16074666e-03 2.07481836e-04 ... 2.12757702e-03\n",
      "  6.22184677e-04 1.23495543e-04]\n",
      " [3.90957553e-01 9.35583870e-03 5.36319749e-04 ... 8.80990053e-03\n",
      "  1.23887156e-03 1.47123582e-04]\n",
      " ...\n",
      " [1.00282027e-03 5.39280587e-04 9.95932942e-05 ... 3.35278199e-01\n",
      "  1.53386400e-02 4.43879991e-04]\n",
      " [9.59120596e-03 1.50494327e-03 4.77777597e-05 ... 2.71410443e-01\n",
      "  4.61185758e-02 4.16548997e-03]\n",
      " [4.73688351e-03 3.06019500e-03 1.89412150e-04 ... 2.55850213e-01\n",
      "  5.10010010e-02 6.79807957e-03]]\n",
      "Extracted Features Shape:  [[1.55465861e-01 3.00801634e-02 6.13251260e-04 ... 5.63965316e-03\n",
      "  9.22320366e-04 1.39542014e-04]\n",
      " [1.90205686e-01 7.16074666e-03 2.07481836e-04 ... 2.12757702e-03\n",
      "  6.22184677e-04 1.23495543e-04]\n",
      " [3.90957553e-01 9.35583870e-03 5.36319749e-04 ... 8.80990053e-03\n",
      "  1.23887156e-03 1.47123582e-04]\n",
      " ...\n",
      " [1.00282027e-03 5.39280587e-04 9.95932942e-05 ... 3.35278199e-01\n",
      "  1.53386400e-02 4.43879991e-04]\n",
      " [9.59120596e-03 1.50494327e-03 4.77777597e-05 ... 2.71410443e-01\n",
      "  4.61185758e-02 4.16548997e-03]\n",
      " [4.73688351e-03 3.06019500e-03 1.89412150e-04 ... 2.55850213e-01\n",
      "  5.10010010e-02 6.79807957e-03]]\n"
     ]
    }
   ],
   "source": [
    "#Import functions from the files\n",
    "import Filtering\n",
    "import Common_average_reference\n",
    "import CCA_Feature_Extraction\n",
    "import numpy as np\n",
    "\n",
    "#Combine all datasets\n",
    "data_total = np.concatenate((data1, data2, data3), axis=2)\n",
    "labels = np.concatenate((np.full(data1.shape[-1], 0),\n",
    "                         np.full(data2.shape[-1], 1),\n",
    "                         np.full(data3.shape[-1], 2)))\n",
    "\n",
    "# Define filtering parameters\n",
    "order = 3                # Define filter order\n",
    "notch_freq = 50          # Define frequency to be removed from the signal for notch filter (Hz)\n",
    "quality_factor = 20      # Define quality factor for the notch filter\n",
    "subbands = [[12, 16, 20], [14, 18, 22]]\n",
    "f_low = np.min(subbands) - 1  # Define lower cutoff frequency for the bandpass filter (Hz)\n",
    "f_high = np.max(subbands) + 1  # Define upper cutoff frequency for the bandpass filter (Hz)\n",
    "notch_filter = \"on\"       # on or off\n",
    "filter_active = \"on\"      # on or off\n",
    "type_filter = \"bandpass\"  # low, high, bandpass, or bandstop\n",
    "\n",
    "# Apply notch filter to the EEG data\n",
    "filtered_data = Filtering.filtering(data_total, f_low, f_high, order, fs,\n",
    "                                     notch_freq, quality_factor,\n",
    "                                     filter_active, notch_filter, type_filter)\n",
    "\n",
    "# Perform Common Average Reference (CAR)\n",
    "data_car = Common_average_reference.car(filtered_data)\n",
    "\n",
    "# Define parameters for feature extraction\n",
    "num_channel = [0, 1, 2]   # List of channels to use\n",
    "num_harmonic = 2          # Number of harmonics\n",
    "f_stim = [13, 21, 17]     # Frequencies stimulation\n",
    "\n",
    "title = f\"Feature Extraction using CCA\"\n",
    "\n",
    "# Perform CCA feature extraction\n",
    "features_extraction = CCA_Feature_Extraction.cca_feature_extraction(data_car, fs, f_stim, num_channel, num_harmonic)\n",
    "\n",
    "# Print or visualize the extracted features\n",
    "print(\"Extracted Features: \", features_extraction)\n",
    "print(\"Extracted Features Shape: \", features_extraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9SZE1aAV_8D6"
   },
   "source": [
    "Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OpejSg62_74i",
    "outputId": "57927e35-e7a5-425d-8c3e-30f5342d5e72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features shape: (480, 4)\n"
     ]
    }
   ],
   "source": [
    "# Import the feature selection function from the uploaded file\n",
    "from Feature_selections import feature_selecions\n",
    "\n",
    "# Define parameters for feature selection\n",
    "num_features = 4\n",
    "n_neighbors_MI = 5                 # Number of neighbors to consider for mutual information calculation.\n",
    "L1_Parameter = 0.1                 # Parameter value for L1 regularization.\n",
    "threshold_var = 0.001              # The threshold used for variance thresholding.\n",
    "type_feature_selection = \"anova\"    # Options: var, anova, mi, ufs, rfe, rf, l1fs, tfs, fs, ffs, bfs\n",
    "title = f\"Feature selection using {type_feature_selection}\"\n",
    "\n",
    "# Perform feature selection\n",
    "features = feature_selecions(features_extraction, labels, num_features, threshold_var,\n",
    "                              n_neighbors_MI, L1_Parameter, type_feature_selection)\n",
    "\n",
    "# Display the selected features\n",
    "print(f\"Selected features shape: {features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uczhDeR7CXl8"
   },
   "source": [
    "Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fZYQha0Z_72Z",
    "outputId": "4cae88b5-f799-499a-fd49-32bac8fc8d59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SVM ---\n",
      "Accuracy: 0.96\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.91      0.95        34\n",
      "           1       0.87      1.00      0.93        27\n",
      "           2       1.00      0.97      0.99        35\n",
      "\n",
      "    accuracy                           0.96        96\n",
      "   macro avg       0.96      0.96      0.96        96\n",
      "weighted avg       0.96      0.96      0.96        96\n",
      "\n",
      "Xtest = (96, 3)\n",
      "Xtrain = (384, 3)\n",
      "Predicted Action: 13\n",
      "Predicted Action: 17\n",
      "Predicted Action: 17\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Assuming `features_extraction` and `labels` are already available from the previous steps\n",
    "\n",
    "# Step 2: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features_extraction, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# Step 3: Standardize the features (important for models like SVM)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Step 4: Train classifiers (Logistic Regression, SVM, and Random Forest)\n",
    "\n",
    "# Logistic Regression\n",
    "logreg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Support Vector Machine\n",
    "svm_model = SVC(kernel='linear', probability=True, random_state=42)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Evaluate models on the test set\n",
    "models = {#\"Logistic Regression\": logreg,\n",
    "           \"SVM\": svm_model\n",
    "          #  , \"Random Forest\": rf_model\n",
    "          }\n",
    "\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"--- {name} ---\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Step 6: Define action mapping based on predictions\n",
    "def action_mapping(prediction):\n",
    "    actions = {\n",
    "        0: \"13\",\n",
    "        1: \"21\",\n",
    "        2: \"17\"\n",
    "    }\n",
    "    return actions.get(prediction, \"Unknown Action\")\n",
    "\n",
    "print(f\"Xtest = {X_test.shape}\")\n",
    "print(f\"Xtrain = {X_train.shape}\")\n",
    "\n",
    "\n",
    "# Step 7: Test the model with some example data\n",
    "for i in range (features.shape[1]):\n",
    "    sample = X_test[i].reshape(1, -1)  # Example data point\n",
    "    predicted_label = rf_model.predict(sample)[0]\n",
    "    predicted_action = action_mapping(predicted_label)\n",
    "\n",
    "    print(f\"Predicted Action: {predicted_action}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEPLOYMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1280, 8)\n",
      "(1280, 8)\n",
      "(1280, 8)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the .npy file\n",
    "data1 = np.load('trials/13hz.npy') [:,:,0]\n",
    "data2 = np.load('trials/21hz.npy')[:,:,0]\n",
    "data3 = np.load('trials/17hz.npy')[:,:,0]\n",
    "\n",
    "\n",
    "# Print the loaded data\n",
    "\n",
    "''' \n",
    "socket= func(pc2 data store) \n",
    "\n",
    "model <- socket \n",
    "0, 1, 2 -> daq\n",
    "''' \n",
    "\n",
    "# print(data1[ :,: ,0].shape)\n",
    "print(data1.shape)\n",
    "print(data2.shape)\n",
    "print(data3.shape)\n",
    "\n",
    "\n",
    "def classification(data):\n",
    "    return None\n",
    "\n",
    "result = classification(data1) # expected result = data1 : 0 (13) , data2 : 1 (21), data3 : 2 (17)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4sZTonSUAQK"
   },
   "source": [
    "Advanced hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5Ut2bqV6OvC5",
    "outputId": "2330be6c-d18a-436c-8c3d-b7c6fd93f14c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stratified Cross-Validation Accuracy: 0.92 ± 0.01\n",
      "--- SVM ---\n",
      "Accuracy: 0.89\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.94      0.85        32\n",
      "           1       0.92      0.75      0.83        32\n",
      "           2       1.00      0.97      0.98        32\n",
      "\n",
      "    accuracy                           0.89        96\n",
      "   macro avg       0.90      0.89      0.89        96\n",
      "weighted avg       0.90      0.89      0.89        96\n",
      "\n",
      "--- Random Forest ---\n",
      "Accuracy: 0.90\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.91      0.85        32\n",
      "           1       0.93      0.81      0.87        32\n",
      "           2       0.97      0.97      0.97        32\n",
      "\n",
      "    accuracy                           0.90        96\n",
      "   macro avg       0.90      0.90      0.90        96\n",
      "weighted avg       0.90      0.90      0.90        96\n",
      "\n",
      "--- XGBoost ---\n",
      "Accuracy: 0.90\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.91      0.85        32\n",
      "           1       0.90      0.81      0.85        32\n",
      "           2       1.00      0.97      0.98        32\n",
      "\n",
      "    accuracy                           0.90        96\n",
      "   macro avg       0.90      0.90      0.90        96\n",
      "weighted avg       0.90      0.90      0.90        96\n",
      "\n",
      "--- SGD ---\n",
      "Accuracy: 0.81\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.97      0.78        32\n",
      "           1       0.96      0.69      0.80        32\n",
      "           2       1.00      0.78      0.88        32\n",
      "\n",
      "    accuracy                           0.81        96\n",
      "   macro avg       0.87      0.81      0.82        96\n",
      "weighted avg       0.87      0.81      0.82        96\n",
      "\n",
      "--- Voting Classifier ---\n",
      "Accuracy: 0.91\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.94      0.87        32\n",
      "           1       0.93      0.81      0.87        32\n",
      "           2       1.00      0.97      0.98        32\n",
      "\n",
      "    accuracy                           0.91        96\n",
      "   macro avg       0.91      0.91      0.91        96\n",
      "weighted avg       0.91      0.91      0.91        96\n",
      "\n",
      "Precision-Recall AUC: 0.96\n",
      "Predicted Action: Leg Movement (21Hz)\n",
      "Prediction Probabilities: [0.4614282  0.49397687 0.04459493]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_recall_curve, auc\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE  # For handling class imbalance\n",
    "\n",
    "# Assuming 'features' and 'labels' are available from feature selection\n",
    "\n",
    "# Step 2: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "# Step 3: Handle Class Imbalance with SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "# Step 4: Standardize the features (important for SVM and Logistic Regression)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Step 5: Define Hyperparameter Grids for Tuning\n",
    "svm_param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf'], 'gamma': ['scale', 'auto']}\n",
    "rf_param_grid = {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20], 'min_samples_split': [2, 5]}\n",
    "\n",
    "# Step 6: Perform Hyperparameter Tuning\n",
    "svm_grid = GridSearchCV(SVC(probability=True, random_state=42), svm_param_grid, cv=5, scoring='accuracy')\n",
    "rf_grid = GridSearchCV(RandomForestClassifier(random_state=42), rf_param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit models with the best hyperparameters\n",
    "svm_grid.fit(X_train, y_train)\n",
    "rf_grid.fit(X_train, y_train)\n",
    "\n",
    "best_svm = svm_grid.best_estimator_\n",
    "best_rf = rf_grid.best_estimator_\n",
    "\n",
    "# Step 7: Define Advanced and Ensemble Models\n",
    "xgb_model = XGBClassifier(eval_metric='mlogloss', random_state=42)  # Removed use_label_encoder\n",
    "sgd_model = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42)\n",
    "\n",
    "# Train the models\n",
    "xgb_model.fit(X_train, y_train)\n",
    "sgd_model.partial_fit(X_train, y_train, classes=np.unique(y_train))\n",
    "\n",
    "# Ensemble using Voting Classifier (soft voting)\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('svm', best_svm), ('rf', best_rf), ('xgb', xgb_model)], voting='soft'\n",
    ")\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Step 8: Evaluate Models with Cross-Validation\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "scores = cross_val_score(voting_clf, X_train, y_train, cv=skf)\n",
    "print(f\"Stratified Cross-Validation Accuracy: {scores.mean():.2f} ± {scores.std():.2f}\")\n",
    "\n",
    "# Step 9: Evaluate on the Test Set\n",
    "models = {\n",
    "    \"SVM\": best_svm,\n",
    "    \"Random Forest\": best_rf,\n",
    "    \"XGBoost\": xgb_model,\n",
    "    \"SGD\": sgd_model,\n",
    "    \"Voting Classifier\": voting_clf\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"--- {name} ---\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Step 10: Evaluate with Precision-Recall AUC\n",
    "y_prob = voting_clf.predict_proba(X_test)[:, 1]\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_prob, pos_label=1)\n",
    "pr_auc = auc(recall, precision)\n",
    "print(f\"Precision-Recall AUC: {pr_auc:.2f}\")\n",
    "\n",
    "# Step 11: Test with a Sample Data Point\n",
    "def action_mapping(prediction):\n",
    "    actions = {0: \"Hand Movement (13Hz)\", 1: \"Leg Movement (21Hz)\", 2: \"Resting State (17Hz)\"}\n",
    "    return actions.get(prediction, \"Unknown Action\")\n",
    "\n",
    "sample = X_test[0].reshape(1, -1)\n",
    "predicted_label = voting_clf.predict(sample)[0]\n",
    "predicted_action = action_mapping(predicted_label)\n",
    "print(f\"Predicted Action: {predicted_action}\")\n",
    "\n",
    "# Display prediction probabilities for transparency\n",
    "probabilities = voting_clf.predict_proba(sample).flatten()\n",
    "print(f\"Prediction Probabilities: {probabilities}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "exIyj6zPOvAd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "461U7PxIOu8B"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
